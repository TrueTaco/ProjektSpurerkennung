{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a77db10",
   "metadata": {},
   "source": [
    "# Projekt: Erkennung von Spurmarkierungen\n",
    "In diesem Projekt sollen Spurmarkierungen in zwei bestehenden Datensätzen erkannt werden: \n",
    "1. Udacity Nanodegree \"Self-Driving Car Engineer\" (https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)\n",
    "2. KITTI-Datensatz zur Erkennung von Spurmarkierungen (http://www.cvlibs.net/datasets/kitti/eval_road.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29415ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print('Willkommen beim Projekt \"Erkennung von Spurmarkierungen\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530612a",
   "metadata": {},
   "source": [
    "# Exkurs 1: Kamerakalibrierung zur Entzerrung der Bilder\n",
    "Die vorhandenen Bilder sind aufgrund der Linsen- und Kameraeigenschaften verzerrt. Entzerren Sie die Bilder mithilfe der Kamerakalibrierungsroutinen von OpenCV (https://docs.opencv.org/4.5.3/dc/dbb/tutorial_py_calibration.html) und den aufgezeichneten Bildern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# einlesen eines Beispielbildes\n",
    "img1 = cv.cvtColor(cv.imread('./img/Udacity/calib/calibration1.jpg'), cv.COLOR_BGR2RGB)\n",
    "plt.imshow(img1)\n",
    "plt.title('Kalibrierungsbild')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65719edd",
   "metadata": {},
   "source": [
    "# Exkurs 2: Perspektivtransformation\n",
    "Durch die Kameraperspektive wird die Krümmung der gefundenen Spurmarkierungen nicht der realen Fahrstreifenkrümmung entsprechen. Transformieren Sie daher die Bilder der Kameraperspektive in eine Vogelperspektive, die der realen Fahrstreifenkrümmung entspricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4109f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# einlesen eines Beispielbildes\n",
    "img1 = cv.imread('./img/Udacity/image001.jpg', -1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "plt.title('Spurmarkierungen')\n",
    "plt.show()\n",
    "\n",
    "# definieren Sie die für eine Perspektivtransformation notwendigen Quell- und Zielpunkte\n",
    "src = np.float32(...)\n",
    "dst = np.float32(...)\n",
    "\n",
    "M = cv.getPerspectiveTransform(src,dst)\n",
    "img1_warp = cv.warpPerspective(img1,M,(img1.shape[1], img1.shape[0]))\n",
    "plt.imshow(cv.cvtColor(img1_warp, cv.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd5355",
   "metadata": {},
   "source": [
    "# Aufgabe 3: Erkennung von Fahrbahnmarkierungen\n",
    "Erkennen Sie die Fahrbahnmarkierungen bzw. Fahrstreifen auf den Bildquellen von Udacity. Wenden Sie dabei die aus der Vorlesung bekannten Verfahren an. Gerne dürfen Sie auch weitere Verfahren aus anderen Quellen verwenden. Folgende Ziele müssen bei der finalen Abgabe erreicht werden: \n",
    "- **Segmentierung**: schränken Sie das Bild auf den Bereich ein, in dem sich die Spurmarkierungen befinden\n",
    "- **Vorverarbeitung**: führen Sie eine Kamerakalibrierung (für Udacity-Bildquellen) und die Perspektivtransformation durch\n",
    "- **Farbräume, Histogramme**: erkennen Sie die Spurmarkierungen in den Farben der angegebenen Quellen. Sofern weitere Spurmarkierungen auf dem Bild gefunden werden, müssen diejenigen Spurmarkierungen priorisiert werden, die die eigene Fahrspur begrenzen\n",
    "- **Allgemeines**: Die Verarbeitung von Bildern muss in Echtzeit stattfinden --> Ziel: > 20 FPS\n",
    "- **Allgemeines**: Beschleunigen Sie die Verarbeitung durch weitere Maßnahmen weitere Maßnahmen überlegen (bspw. Erkennung der Spurmarkierung in den ersten Frames, Tracking der Spurmarkierung in weiteren Frames solange, bis sich Spurmarkierungspositionen zu stark ändern)\n",
    "- **Minimal**: relevante Spurmarkierungen werden im Video \"project_video\" durchgehend erkannt \n",
    "- **Zusatz**: relevante Spurmarkierungen werden im Video \"challenge_video\" und \"harder_challenge_video\" durchgehend erkannt\n",
    "- **Zusatz**: relevante Spurmarkierungen werden auf den Datensatz KITTI angewendet. Welche Anpassungen müssen vorgenommen werden, damit Ihr Algorithmus übertragen werden kann?\n",
    "- **Zusatz**: Erarbeiten Sie weitere Maßnahmen zur Geschwindigkeitsverbesserung Ihres Algorithmus\n",
    "- **Zusatz**: Erkennen Sie Objekte im Bild und visualisieren Sie diese (z.B. weitere Fahrzeuge, Motorräder, etc.)\u000bDie Objekterkennung bitte so implementieren, dass sie deaktivierbar ist und nicht in FPS-Berechnung einzahlt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython import display\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    # Define a blank matrix that matches the image height/width.\n",
    "    mask = np.zeros_like(img)\n",
    "    # Retrieve the number of color channels of the image.\n",
    "    channel_count = img.shape[2]\n",
    "    # Create a match color with the same color channel counts.\n",
    "    match_mask_color = (255,) * channel_count\n",
    "      \n",
    "    # Fill inside the polygon\n",
    "    cv.fillPoly(mask, np.array([vertices],np.int32), match_mask_color)\n",
    "    \n",
    "    # Returning the image only where mask pixels match\n",
    "    masked_image = cv.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def process_frame(frame):\n",
    "    img1_hsv = cv.cvtColor(frame, cv.COLOR_RGB2HSV)\n",
    "    #plt.imshow(frame)\n",
    "    #plt.show()\n",
    "\n",
    "    height = len(img1_hsv)\n",
    "    width = len(img1_hsv[0])\n",
    "\n",
    "    region_of_interest_vertices = [[0, height],[width / 2, height / 2],[width, height],]   \n",
    "\n",
    "    img1_hsv = region_of_interest(img1_hsv, region_of_interest_vertices)\n",
    "\n",
    "    lower_white = np.array([60,0,220], dtype=np.uint8)\n",
    "    upper_white = np.array([110,10,255], dtype=np.uint8)\n",
    "    mask = cv.inRange(img1_hsv, lower_white, upper_white)\n",
    "\n",
    "    img_sign1 = cv.inRange(img1_hsv, (15, 40, 230), (255, 255, 255))\n",
    "\n",
    "    img_sign = img_sign1 + mask\n",
    "    kernel_small5 = np.array([[0,1,0],[1,1,1],[0,1,0]], 'uint8')\n",
    "    img_sign = cv.dilate(img_sign, kernel_small5, iterations=5)\n",
    "\n",
    "    img_filtered = frame.copy()\n",
    "    img_filtered[np.where(img_sign==0)] = 0\n",
    "\n",
    "    \n",
    "\n",
    "    return img_filtered\n",
    "\n",
    "\n",
    "capture = cv.VideoCapture('img/Udacity/project_video.mp4')\n",
    "frameNr = 0\n",
    "while(True):\n",
    "    success, frame = capture.read() \n",
    "    if(success):\n",
    "        processed_img = process_frame(frame)\n",
    "        alpha = 0.1\n",
    "        beta = (1.0 - alpha)\n",
    "        dst = cv.addWeighted(frame, alpha, processed_img, beta, 0.0)\n",
    "        test = frame + processed_img\n",
    "\n",
    "        window = cv.imshow(\"Current Frame\",test)\n",
    "        #display.clear_output(wait=True)\n",
    "        #plt.show()   \n",
    "        frameNr += 1\n",
    "        key = cv.waitKey(1)\n",
    "        if key == ord(\"q\"):\n",
    "            cv.destroyAllWindows()\n",
    "            print(\"Playback interrupted by user.\")\n",
    "            break\n",
    "        if key == ord('p'):\n",
    "            cv.waitKey(-1) #wait until any key is pressed\n",
    "    else:\n",
    "        print(\"Playback finished.\")\n",
    "        capture.release()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ade8db",
   "metadata": {},
   "source": [
    "# Ausblick auf weitere Teilaufgaben\n",
    "- Bestimmung der Kurvenkrümmung anhand von Polynom-Fiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f6feb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "067b04e7",
   "metadata": {},
   "source": [
    "# ToDo`s:\n",
    "- Segmentierung des Bildes:  schränken Sie das Bild auf den Bereich ein, in dem sich die Spurmarkierungen befinden (Michel: Fertig)\n",
    "\n",
    "\n",
    "- Farbräume, Histogramme: erkennen Sie die Spurmarkierungen in den Farben der angegebenen QuellenFalls weitere Spurmarkierungen auf dem Bild gefunden werden, müssen die der eigenen Fahrspur priorisiert werden (Michel: Fertig)\n",
    "\n",
    "\n",
    "- Curve/ Polynom Fitting:Erkennen Sie die Krümmung der Fahrspur und geben Sie diese  im Ausgabebild aus (?: Ausstehend)\n",
    "\n",
    "\n",
    "- Vorverarbeitung: Führen Sie eine Kamerakalibrierung (für Udacity-Bildquellen) und die Perspektivtransformation durch (?: Ausstehend)\n",
    "\n",
    "## Anforderungen \n",
    "- Allgemeines:relevanteSpurmarkierungenwerdenin den Udacity-Bildern und im Video„project_video“ durchgehend erkannt\n",
    "- Allgemeines: Die Verarbeitung von Bildern muss in Echtzeit stattfinden --> Ziel: > 20 FPS\n",
    "- Allgemeines: Beschleunigen Sie die Verarbeitung durch weitere Maßnahmen (bspw. Erkennung der Spurmarkierung in den ersten Frames, Tracking der Spurmarkierung in weiteren Frames solange, bis sich Spurmarkierungspositionen zu stark ändern) →mind. eine Maßnahme im Projekt verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ff9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e692b8c730dce42c75f6e0bb113cc8b09fa56a397f29a247e19fb3548ba9015"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('DigitaleBildverarbeitung': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
